doc.term <- dfm(twcorpus, verbose=TRUE)
doc.term
dim(doc.term)
which_column_num_is_school <- which(colnames(doc.term) == "school") ## it's the 8-th column
colSums(doc.term[,which_column_num_is_school])
doc.term[1:5, 1:10]
doc.term <- dfm(twcorpus, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE, ngrams=1:3, verbose=TRUE)
doc.term
example <- tolower(reviews$text[1])
print (reviews$text[1])
print ("-----------Tokens")
token1 <- tokens(example) #token of the first document
print (token1) # only the first 10 was printed
print (token1, max_ntoken = 1000)
print (token1)
print ("-----------Tokens after stemming")
tokens_wordstem(token1)
tokens_ngrams(token1, 1:3)
doc.term <- dfm_trim(doc.term, min_docfreq=10, verbose=TRUE)
doc.term
tfidf <- dfm_tfidf(doc.term)
topfeatures(doc.term, 25)
textplot_wordcloud(doc.term, rotation=0, min_size=2, max_size=10, max_words=100)
doc.term <- dfm(twcorpus, remove_punct = TRUE, remove=c(
stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"), remove_url=TRUE, verbose=TRUE)
textplot_wordcloud(doc.term, rotation=0, min_size=1, max_size=5, max_words=100)
pos.words <- read.csv("opinion-lexicon-English/positive-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
head(pos.words)
neg.words <- read.csv("opinion-lexicon-English/negative-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
head(neg.words)
mydict <- dictionary(list(positive = pos.words,
negative = neg.words))
# sent <- dfm(twcorpus, dictionary = mydict, remove_punct = TRUE, remove=c(
# stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"),remove_url=TRUE, verbose=TRUE)
sent <- dfm(doc.term, dictionary = mydict)
sent
reviews$score <- ifelse(as.numeric(sent[,1]) > as.numeric(sent[,2]), 1, 0)
table(reviews$label)
table(actual = reviews$label, predicted = reviews$score)
numerator <- as.numeric(sent[,1]) + -1 * as.numeric(sent[,2])
denominator <- ntoken(doc.term) # or use rowSums(); it's the same
sent_continuous <- numerator/denominator
plot(density(sent_continuous))
sent_prob = plogis(sent_continuous) # probability of being 1 (positive)
reviews$sent_prob = sent_prob
library(PRROC)
reviews.positive <- reviews[reviews$label == 1,]
reviews.negative <- reviews[reviews$label == 0,]
pr <- pr.curve(reviews.positive$sent_prob, reviews.negative$sent_prob, curve = TRUE )
plot(pr)
roc <- roc.curve(reviews.positive$sent_prob, reviews.negative$sent_prob, curve = TRUE )
plot(roc)
library(quanteda)
library(data.table) # this package help you to read large files quicker
reviews <- fread("train.csv") # fread() is a function in data.table
twcorpus <- corpus(reviews$text)
summary(twcorpus, n=10)
library(quanteda)
library(data.table) # this package help you to read large files quicker
reviews <- fread("train.csv") # fread() is a function in data.table
View(reviews)
help("corpus")
twcorpus <- corpus(reviews$text)
twcorpus
summary(twcorpus, n=10)
help("summary.corpus")
kwic(x = twcorpus, pattern = "like", window=5, separator = "}")[1:5,]
kwic(x = twcorpus, pattern = "like", window=5, separator = "}")[1:5,]
doc.term <- dfm(x = twcorpus, verbose=TRUE)
doc.term
dim(doc.term)
which_column_num_is_school <- which(colnames(doc.term) == "school")
which_column_num_is_school
print (which_column_num_is_school)
print (sum(doc.term[,which_column_num_is_school]))
doc.term[1:5, 1:10]
help("dfm")
doc.term <- dfm(twcorpus, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE, ngrams=1:3, verbose=TRUE)
stopwords("english")
doc.term <- dfm(twcorpus, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE, ngrams=3, verbose=TRUE)
stopwords("english")
example <- tolower(reviews$text[1])
example
print (reviews$text[1])
print ("-----------Tokens")
token1 <- tokens(example) #token of the first document
token1
print (token1, max_ntoken = 1000)
print (token1)
print ("-----------Tokens after stemming")
tokens_wordstem(token1)
tokens_ngrams(token1, 1:3)
tokens_ngrams[100]
View(tokens_ngrams)
tokens_ngrams(token1, 1:3)
tokens_ngrams(token1, 1:3, n = 1000)
tk <- tokens_ngrams(token1, 1:3)
tk
help(tokens_ngrams)
tokens1
token1
doc.term <- dfm_trim(doc.term, min_docfreq=10, verbose=TRUE)
doc.term
tfidf <- dfm_tfidf(doc.term)
head(tfidf)
topfeatures(doc.term, 25)
textplot_wordcloud(doc.term, rotation=0, min_size=2, max_size=10, max_words=100)
pos.words <- read.csv("opinion-lexicon-English/positive-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
pos.words <- read.csv("opinion-lexicon-English/positive-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
head(pos.words)
neg.words <- read.csv("opinion-lexicon-English/negative-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
head(neg.words)
mydict <- dictionary(list(positive = pos.words,
negative = neg.words))
mydict
# sent <- dfm(twcorpus, dictionary = mydict, remove_punct = TRUE, remove=c(
# stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"),remove_url=TRUE, verbose=TRUE)
sent <- dfm(doc.term, dictionary = mydict)
dfm
help(dfm)
# sent <- dfm(twcorpus, dictionary = mydict, remove_punct = TRUE, remove=c(
# stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"),remove_url=TRUE, verbose=TRUE)
sent <- dfm(doc.term, dictionary = mydict)
sent
reviews$score <- ifelse(as.numeric(sent[,1]) > as.numeric(sent[,2]), 1, 0)
reviews$score <- ifelse(as.numeric(sent[,1]) > as.numeric(sent[,2]), 1, 0)
View(reviews)
table(reviews$label)
table(actual = reviews$label, predicted = reviews$score)
numerator <- as.numeric(sent[,1]) + -1 * as.numeric(sent[,2])
denominator <- ntoken(doc.term) # or use rowSums(); it's the same
numerator <- as.numeric(sent[,1]) + -1 * as.numeric(sent[,2])
denominator <- ntoken(doc.term) # or use rowSums(); it's the same
sent_continuous <- numerator/denominator
sent_continuous
as.numeric(sent_continuous)
sent_prob = plogis(sent_continuous) # probability of being 1 (positive)
reviews$sent_prob = sent_prob
View(reviews)
library(PRROC)
reviews.positive <- reviews[reviews$label == 1,]
reviews.negative <- reviews[reviews$label == 0,]
pr <- pr.curve(reviews.positive$sent_prob, reviews.negative$sent_prob, curve = TRUE )
plot(pr)
roc <- roc.curve(reviews.positive$sent_prob, reviews.negative$sent_prob, curve = TRUE )
plot(roc)
set.seed(12345)
anes <- read_delim("anes_timeseries_2016_rawdata.txt", delim = "|") %>%
select(vote = V162034a, V161270, gender = V161342, age = V161267) %>%
mutate_all(as.numeric) %>%
filter(vote %in% c(1,2) & gender %in% c(1,2)) %>%
mutate(vote = factor(vote, levels = 1:2, labels = c("Clinton", "Trump")),
educ = case_when(
V161270 %in% 1:8 ~ 1,
V161270 %in% 9 ~ 2,
V161270 %in% 10:12 ~ 3,
V161270 %in% 13 ~ 4,
V161270 %in% 14:16 ~ 5,
TRUE ~ -999),
gender = factor(gender, levels = 1:2, labels = c("Male", "Female"))) %>%
mutate(educ = factor(educ, level = 1:5, labels = c("HS Not Completed",
"Completed HS",
"College < 4 Years",
"College 4 Year Degree",
"Advanced Degree"))) %>%
filter(!is.na(educ) & age >= 18) %>%
dplyr::select(-V161270)
# load some required packages
library(ggplot2)
library(reshape2)
library(nlme)
library(ISLR)
library(foreign)
library(AER)
library(MASS)
library(tidyverse)
library(ggplot2)
library(knitr)
library(boot)
library(texreg)
set.seed(12345)
anes <- read_delim("anes_timeseries_2016_rawdata.txt", delim = "|") %>%
select(vote = V162034a, V161270, gender = V161342, age = V161267) %>%
mutate_all(as.numeric) %>%
filter(vote %in% c(1,2) & gender %in% c(1,2)) %>%
mutate(vote = factor(vote, levels = 1:2, labels = c("Clinton", "Trump")),
educ = case_when(
V161270 %in% 1:8 ~ 1,
V161270 %in% 9 ~ 2,
V161270 %in% 10:12 ~ 3,
V161270 %in% 13 ~ 4,
V161270 %in% 14:16 ~ 5,
TRUE ~ -999),
gender = factor(gender, levels = 1:2, labels = c("Male", "Female"))) %>%
mutate(educ = factor(educ, level = 1:5, labels = c("HS Not Completed",
"Completed HS",
"College < 4 Years",
"College 4 Year Degree",
"Advanced Degree"))) %>%
filter(!is.na(educ) & age >= 18) %>%
dplyr::select(-V161270)
set.seed(12345)
anes <- read_delim("anes_timeseries_2016_rawdata.txt", delim = "|") %>%
select(vote = V162034a, V161270, gender = V161342, age = V161267) %>%
mutate_all(as.numeric) %>%
filter(vote %in% c(1,2) & gender %in% c(1,2)) %>%
mutate(vote = factor(vote, levels = 1:2, labels = c("Clinton", "Trump")),
educ = case_when(
V161270 %in% 1:8 ~ 1,
V161270 %in% 9 ~ 2,
V161270 %in% 10:12 ~ 3,
V161270 %in% 13 ~ 4,
V161270 %in% 14:16 ~ 5,
TRUE ~ -999),
gender = factor(gender, levels = 1:2, labels = c("Male", "Female"))) %>%
mutate(educ = factor(educ, level = 1:5, labels = c("HS Not Completed",
"Completed HS",
"College < 4 Years",
"College 4 Year Degree",
"Advanced Degree"))) %>%
filter(!is.na(educ) & age >= 18) %>%
dplyr::select(-V161270)
trump_model <- glm(vote ~ gender + educ + age, data = anes,
family = binomial(link = "logit"))
screenreg(trump_model)
cis <- exp(confint(trump_model)) %>%
as.data.frame %>%
rownames_to_column("Variable")
cis$OR = exp(coef(trump_model))
ggplot(data = cis, aes(x = Variable, y = `OR`, ymin = `2.5 %`, ymax = `97.5 %`)) +
geom_pointrange() +
geom_hline(yintercept = 1, lty = 2) +
coord_flip() +
xlab("Variable") + ylab("Odds Ratio with 95% CI")
help(anes)
set.seed(12345)
anes <- read_delim("anes_timeseries_2016_rawdata.txt", delim = "|") %>%
select(vote = V162034a, V161270, gender = V161342, age = V161267) %>%
mutate_all(as.numeric) %>%
filter(vote %in% c(1,2) & gender %in% c(1,2)) %>%
mutate(vote = factor(vote, levels = 1:2, labels = c("Clinton", "Trump")),
educ = case_when(
V161270 %in% 1:8 ~ 1,
V161270 %in% 9 ~ 2,
V161270 %in% 10:12 ~ 3,
V161270 %in% 13 ~ 4,
V161270 %in% 14:16 ~ 5,
TRUE ~ -999),
gender = factor(gender, levels = 1:2, labels = c("Male", "Female"))) %>%
mutate(educ = factor(educ, level = 1:5, labels = c("HS Not Completed",
"Completed HS",
"College < 4 Years",
"College 4 Year Degree",                                      "Advanced Degree"))) %>%
filter(!is.na(educ) & age >= 18) %>%
dplyr::select(-V161270)
trump_model <- glm(vote ~ gender + educ + age, data = anes,
family = binomial(link = "logit"))
screenreg(trump_model)
cis <- exp(confint(trump_model)) %>%
as.data.frame %>%
rownames_to_column("Variable")
cis$OR = exp(coef(trump_model))
ggplot(data = cis, aes(x = Variable, y = `OR`, ymin = `2.5 %`, ymax = `97.5 %`)) +
geom_pointrange() +
geom_hline(yintercept = 1, lty = 2) +
coord_flip() +
xlab("Variable") + ylab("Odds Ratio with 95% CI")
head(anes)
set.seed(12345)
anes <- read_delim("anes_timeseries_2016_rawdata.txt", delim = "|") %>%
select(vote = V162034a, V161270, gender = V161342, age = V161267) %>%
mutate_all(as.numeric) %>%
filter(vote %in% c(1,2) & gender %in% c(1,2)) %>%
mutate(vote = factor(vote, levels = 1:2, labels = c("Clinton", "Trump")),
educ = case_when(
V161270 %in% 1:8 ~ 1,
V161270 %in% 9 ~ 2,
V161270 %in% 10:12 ~ 3,
V161270 %in% 13 ~ 4,
V161270 %in% 14:16 ~ 5,
TRUE ~ -999),
gender = factor(gender, levels = 1:2, labels = c("Male", "Female"))) %>%
mutate(educ = factor(educ, level = 1:5, labels = c("HS Not Completed",
"Completed HS",
"College < 4 Years",
"College 4 Year Degree",
"Advanced Degree"))) %>%
filter(!is.na(educ) & age >= 18) %>%
dplyr::select(-V161270)
trump_model <- glm(vote ~ gender + educ + age, data = anes,
family = binomial(link = "logit"))
screenreg(trump_model)
cis <- exp(confint(trump_model)) %>%
as.data.frame %>%
rownames_to_column("Variable")
cis$OR = exp(coef(trump_model))
ggplot(data = cis, aes(x = Variable, y = `OR`, ymin = `2.5 %`, ymax = `97.5 %`)) +
geom_pointrange() +
geom_hline(yintercept = 1, lty = 2) +
coord_flip() +
xlab("Variable") + ylab("Odds Ratio with 95% CI")
head(anes)
table(anes$educ)
trump_model <- glm(vote ~ gender + educ + age, data = anes,
family = binomial(link = "logit"))
screenreg(trump_model)
cis <- exp(confint(trump_model)) %>%
as.data.frame %>%
rownames_to_column("Variable")
cis
confint(trump_model)
ggplot(data = cis, aes(x = Variable, y = `OR`, ymin = `2.5 %`, ymax = `97.5 %`)) +
geom_pointrange() +
geom_hline(yintercept = 1, lty = 2) +
coord_flip() +
xlab("Variable") + ylab("Odds Ratio with 95% CI")
cis <- exp(confint(trump_model)) %>%
as.data.frame %>%
rownames_to_column("Variable")
cis <- exp(confint(trump_model)) %>%
as.data.frame %>%
rownames_to_column("Variable")
cis$OR = exp(coef(trump_model))
ggplot(data = cis, aes(x = Variable, y = `OR`, ymin = `2.5 %`, ymax = `97.5 %`)) +
geom_pointrange() +
geom_hline(yintercept = 1, lty = 2) +
coord_flip() +
xlab("Variable") + ylab("Odds Ratio with 95% CI")
dim(anes)
# sample the same number
sample (nrow(anes), size = nrow(anes), replace = TRUE)
# sample the same number
sample (nrow(anes), size = nrow(anes), replace = TRUE)
nrow(anes)
# sample the same number
ind <- sample (nrow(anes), size = nrow(anes), replace = TRUE)
ind
table(ind)
# sample the same number
# ind <- sample (nrow(anes), size = nrow(anes), replace = TRUE)
anes_new = anes[sample (nrow(anes), size = nrow(anes), replace = TRUE), ]
dim(anes)
dim(anes_new)
logistic_bootstrap = glm(vote ~ gender + educ + age, data = anes_new,
family = binomial(link = "logit"))
coef(logistic_bootstrap)
logistic_estimate <- c()
logistic_estimate
c(coef(logistic_bootstrap), exp(coef(logistic_bootstrap)))
logistic_estimate <- c()
for (i in 1:1000){
# sample the same number
# ind <- sample (nrow(anes), size = nrow(anes), replace = TRUE)
anes_new = anes[sample (nrow(anes), size = nrow(anes), replace = TRUE), ]
# estimate regression with the bootstrap sample
logistic_bootstrap = glm(vote ~ gender + educ + age, data = anes_new,
family = binomial(link = "logit"))
# save coefficient for wage
logistic_estimate = rbind(logistic_estimate, c(coef(logistic_bootstrap), exp(coef(logistic_bootstrap))))
}
head(logistic_estimate)
View(logistic_estimate)
help("apply")
# standard errors
std.boot <- apply(logistic_estimate, 2, sd)
# average
mu <- apply(logistic_estimate, 2, mean)
lower <- apply(logistic_estimate, 2, function(x)quantile(x, 0.025))
upper <- apply(logistic_estimate, 2, function(x)quantile(x, 0.975))
# average
mu <- apply(logistic_estimate, 2, mean)
# standard errors
std.boot <- apply(logistic_estimate, 2, sd)
mu
std.boot <- apply(logistic_estimate, 2, sd)
std.boot
trump_model
summary(trump_model)
# bootstrap estimate
lower <- apply(logistic_estimate, 2, function(x)quantile(x, 0.025))
upper <- apply(logistic_estimate, 2, function(x)quantile(x, 0.975))
# average
mu <- apply(logistic_estimate, 2, mean)
# standard errors
std.boot <- apply(logistic_estimate, 2, sd)
print (std.boot)
#
#
cis.boot <- data.frame(Variable = cis$Variable, OR = mu, ymin = lower, ymax = upper)
summary(trump_model)
#
#
cis.boot <- data.frame(Variable = cis$Variable, mean = mu, ymin = lower, ymax = upper)
cis.boot
# bootstrap estimate
lower <- apply(logistic_estimate, 2, function(x)quantile(x, 0.025))
upper <- apply(logistic_estimate, 2, function(x)quantile(x, 0.975))
# average
mu <- apply(logistic_estimate, 2, mean)
# standard errors
std.boot <- apply(logistic_estimate, 2, sd)
print (std.boot)
#
#
cis.boot <- data.frame(Variable = cis$Variable, mean = mu, ymin = lower, ymax = upper)
print (cis.boot)
conf(trump_model)
confint(trump_model)
fixdata = data.frame(wage = c(50, 100, 150,200, 250, 300), education = "5. Advanced Degree", race = "1. White")
# we directly add type = "response"; it outputs predicted probability directly, not log odds
probability = predict(logistic, fixdata, type = "response")
fixdata = data.frame(wage = c(50, 100, 150,200, 250, 300), education = "5. Advanced Degree", race = "1. White")
# we directly add type = "response"; it outputs predicted probability directly, not log odds
probability = predict(trump_model, fixdata, type = "response")
library(quanteda)
library(data.table) # this package help you to read large files quicker
reviews <- fread("train.csv") # fread() is a function in data.table
head(reviews)
twcorpus <- corpus(reviews$text)
summary(twcorpus, n=10)
kwic(x = twcorpus, pattern = "like", window=5, separator = "}")[1:5,]
kwic(x = twcorpus, pattern = "like", window=5, separator = ",")[1:5,]
doc.term <- dfm(x = twcorpus, verbose=TRUE)
doc.term
dim(doc.term)
which_column_num_is_school <- which(colnames(doc.term) == "school")
print (which_column_num_is_school)
print (sum(doc.term[,which_column_num_is_school]))
doc.term[1:5, 1:10]
doc.term <- dfm(twcorpus, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE, verbose=TRUE)
doc.term
example <- tolower(reviews$text[1])
print (reviews$text[1])
print ("-----------Tokens")
token1 <- tokens(example) #token of the first document
print (token1) # only the first 10 was printed
print (token1, max_ntoken = 1000)
print (token1)
print ("-----------Tokens after stemming")
tokens_wordstem(token1)
tk <- tokens_ngrams(token1, 1:3)
tk
tail(tk)
tk
View(tk)
doc.term <- dfm_trim(doc.term, min_docfreq=10, verbose=TRUE)
doc.term
tfidf <- dfm_tfidf(doc.term)
topfeatures(doc.term, 25)
textplot_wordcloud(doc.term, rotation=0, min_size=2, max_size=10, max_words=100)
doc.term <- dfm(twcorpus, remove_punct = TRUE, remove=c(
stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"), remove_url=TRUE, verbose=TRUE)
textplot_wordcloud(doc.term, rotation=0, min_size=1, max_size=5, max_words=100)
tk
token1
tk$text1
str(tk)
tk
tk[500]
tk[500,]
tk[,500]
tk[[1]]
token1[[1]]
library(quanteda)
library(data.table) # this package help you to read large files quicker
reviews <- fread("train.csv") # fread() is a function in data.table
library(quanteda)
library(data.table) # this package help you to read large files quicker
reviews <- fread("train.csv") # fread() is a function in data.table
read.csv("train.csv")
reviews <- fread("train.csv") # fread() is a function in data.table
View(reviews)
twcorpus <- corpus(reviews$text)
summary(twcorpus, n=10)
kwic(x = twcorpus, pattern = "like", window=5, separator = ",")[1:5,]
doc.term <- dfm(x = twcorpus, verbose=TRUE)
doc.term
help(dfm)
which_column_num_is_school <- which(colnames(doc.term) == "school")
print (which_column_num_is_school)
print (sum(doc.term[,which_column_num_is_school]))
doc_term[,18]
doc.term[,18]
doc.term[1:5, 1:10]
doc.term <- dfm(twcorpus, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE, verbose=TRUE)
doc.term
example <- tolower(reviews$text[1])
print (reviews$text[1])
print ("-----------Tokens")
token1 <- tokens(example) #token of the first document
print (token1) # only the first 10 was printed
token1[[1]]
tk <- tokens_ngrams(token1, 1:3)
tk[[1]]
doc.term <- dfm_trim(doc.term, min_docfreq=10, verbose=TRUE)
doc.term
doc.term <- dfm_trim(doc.term, min_docfreq=10, verbose=TRUE)
doc.term
tfidf <- dfm_tfidf(doc.term)
tfidf <- dfm_tfidf(doc.term)
tfidf
topfeatures(doc.term, 25)
stopwords("english")
View(stopwords("english"))
doc.term <- dfm(twcorpus, remove_punct = TRUE, remove=c(
stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"), remove_url=TRUE, verbose=TRUE)
doc.term
