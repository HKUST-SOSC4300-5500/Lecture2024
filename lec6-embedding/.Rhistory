library(quanteda)
library(Rtsne)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
# text to corpus
twcorpus <- corpus(reviews$text)
# the below commented lien won't work; read the help file
# doc.term <- dfm(twcorpus,  ngrams=1:3, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE,  verbose=TRUE, remove=c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"))
# transform corpus to tokens
tokens <-  tokens(twcorpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE)
tokens <- tokens_remove(tokens, c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br", " "))
tokens <- tokens_wordstem(tokens)
tokens <- tokens_ngrams(tokens, n = 1, concatenator = ".")
# then generate document-
doc.term <- dfm(tokens,  verbose=TRUE)
print (dim(doc.term))
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=50, verbose=TRUE)
print (dim(doc.term))
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=100, verbose=TRUE)
print (dim(doc.term))
X <- as.matrix(doc.term)
dim(X)
library(gmodels)
help(fast.pccomp)
??fast.prcomp
library(quanteda)
library(Rtsne)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
# text to corpus
twcorpus <- corpus(reviews$text)
# the below commented lien won't work; read the help file
# doc.term <- dfm(twcorpus,  ngrams=1:3, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE,  verbose=TRUE, remove=c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"))
# transform corpus to tokens
tokens <-  tokens(twcorpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE)
tokens <- tokens_remove(tokens, c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br", " "))
tokens <- tokens_wordstem(tokens)
tokens <- tokens_ngrams(tokens, n = 1, concatenator = ".")
# then generate document-
doc.term <- dfm(tokens,  verbose=TRUE)
print (dim(doc.term))
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=200, verbose=TRUE)
print (dim(doc.term))
library(quanteda)
library(Rtsne)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
# text to corpus
twcorpus <- corpus(reviews$text)
# the below commented lien won't work; read the help file
# doc.term <- dfm(twcorpus,  ngrams=1:3, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE,  verbose=TRUE, remove=c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"))
# transform corpus to tokens
tokens <-  tokens(twcorpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE)
tokens <- tokens_remove(tokens, c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br", " "))
tokens <- tokens_wordstem(tokens)
tokens <- tokens_ngrams(tokens, n = 1, concatenator = ".")
# then generate document-
doc.term <- dfm(tokens,  verbose=TRUE)
print (dim(doc.term))
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=200, verbose=TRUE)
print (dim(doc.term))
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=500, verbose=TRUE)
X <- as.matrix(doc.term)
dim(X)
library(gmodels)
pc <- fast.prcomp(X, retx = TRUE, center = TRUE, scale. = FALSE, tol = NULL)
dim(pc)
X_subset = X[1:1000, ]
dim(X_subset)
pc <- fast.prcomp(X_subset, retx = TRUE, center = TRUE, scale. = FALSE, tol = NULL)
str(pc)
dim(pc)
pc$rotation
dim(pc$rotation)
dim(pc$x)
dim(pc$rotation)
View(pc$rotation)
dim(X_subset = X[1:1000, ])
dim(X_subset)
View(X_subset)
plot(pc$rotation[, 1],  pc$rotation[,2])
pc$rotation[pc$rotation[,1] > 0.5]
pc$rotation[pc$rotation[,1] > 0.5, ]
which(pc$rotation[,1] > 0.5)
which(pc$rotation[,1] > 0.2)
source ("fast_tsne.R")
tsne <- fftRtsne(X_subset, dims = 2, max_iter = 50)
tsne <- Rtsne(X_subset, dims = 2, partial_pca = T, perplexity=30, verbose=TRUE, max_iter = 50)
install.packages("irlba")
tsne <- Rtsne(X_subset, dims = 2, partial_pca = T, perplexity=30, verbose=TRUE, max_iter = 50)
plot(tsne)
str(tsne)
dim(tsne$Y)
embedding <- tsne$Y
plot(embedding[,1], embedding[,2])
head(embedding)
library(quanteda)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
# text to corpus
twcorpus <- corpus(reviews$text)
# the below commented lien won't work; read the help file
# doc.term <- dfm(twcorpus,  ngrams=1:3, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE,  verbose=TRUE, remove=c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"))
# transform corpus to tokens
tokens <-  tokens(twcorpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE)
tokens <- tokens_remove(tokens, c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br", " "))
tokens <- tokens_wordstem(tokens)
tokens <- tokens_ngrams(tokens, n = 1:2, concatenator = ".")
# then generate document-
doc.term <- dfm(tokens,  verbose=TRUE)
print (dim(doc.term))
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=50, verbose=TRUE)
print (dim(doc.term))
# n_train is the number of training data
n_test <- round (nrow(doc.term) * 0.2)
n_train <- round(nrow(doc.term) * 0.7)
# first set up test data
test_index <- sample(1:nrow(doc.term), n_test, replace = FALSE)
# what is left is the training and validation data
train_and_val_index <- (1:nrow(doc.term))[-test_index]
# further sample training data within (train and validation data)
train_index <- sample(train_and_val_index, n_train, replace = FALSE)
# the rest is test data
val_index <- setdiff(train_and_val_index, train_index)
## get train, val, and test data
test_data <- doc.term[test_index, ]
train_data <- doc.term[train_index, ]
val_data <- doc.term[val_index, ]
train_and_val_data <- doc.term[train_and_val_index, ]
test_Y <- reviews[test_index]$label
train_Y <- reviews[train_index]$label
val_Y <- reviews[val_index]$label
train_and_val_Y <- reviews[train_and_val_index]$label
X <- convert(doc.term, to = "data.frame")
X <- as.data.frame(doc.term)
## the first column is doc_id so I remove tat
X <- X[,2:ncol(X)]
# not run; too slow and memory intensive
# m.lm <- lm (reviews$label ~ as.matrix(X))
library(glmnet)
m.lasso <- glmnet(train_and_val_data, train_and_val_Y,
family="binomial", lambda = 0.1, alpha=1, intercept=TRUE)
# print all regression coefficients here
# m.lasso$beta
m.lasso <- glmnet(train_and_val_data, train_and_val_Y,
family="binomial", nlambda = 20, alpha=1, intercept=TRUE)
print (m.lasso, nlambda = 20)
#m.lasso$beta
plot(m.lasso, xvar = "lambda", label = TRUE)
m.lasso$beta
## function to compute accuracy
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
F1 <- function(ypred, y){
prec <- precision (ypred, y)
rec <- recall (ypred, y)
return ( ( 2 * prec * rec) / (prec + rec))
}
lambda_list <- seq(0,0.01,0.002)
F1.list <- c()
## notice that I have used the newer training data for fitting this model
for (each_lambda in lambda_list){
m.lasso <- glmnet(train_data, train_Y,
family="binomial", lambda = each_lambda, alpha=1)
# predict and evaluate on the validation data
pred_y <- predict(m.lasso, newx = val_data, type="class")
head(pred_y)
F1.list <- c(F1.list, F1(pred_y, val_Y))
}
plot(lambda_list, F1.list)
# use the original training data (without splitting to new train/validation)
# the package will do that for you.
cv.glmnet(train_and_val_data, train_and_val_Y, family="binomial", type.measure = "auc", alpha = 1, nfolds = 5)
reviews
doc.term
m.lasso <- glmnet(test_data, test_Y,
family="binomial", lambda = 0.002233, alpha=1)
# m.lasso$beta
m.lasso$beta
pos.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/positive-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
neg.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/negative-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
mydict <- dictionary(list(positive = pos.words,
negative = neg.words))
pos.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/positive-words.txt", stringsAsFactors = FALSE, header = FALSE)$V1
neg.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/negative-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
mydict <- dictionary(list(positive = pos.words,
negative = neg.words))
pos.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/positive-words.txt", stringsAsFactors = FALSE, header = FALSE)$V1
neg.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/negative-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
mydict <- dictionary(list(positive = pos.words,
negative = neg.words))
pos.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/positive-words.txt", stringsAsFactors = FALSE, header = FALSE)$V1
pos.words
pos.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/positive-words.txt", stringsAsFactors = FALSE, header = FALSE)$V1
neg.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/negative-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
mydict <- dictionary(list(positive = pos.words,
negative = neg.words))
mydict
sent <- dfm(doc.term, dictionary = mydict)
sent_val <- sent[val_index, ]
sent_val
pred_y_dict <- ifelse(as.numeric(sent_val[,1]) > as.numeric(sent_val[,2]), 1, 0)
head(pred_y_dict)
F1(pred_y_dict, val_Y)
X <- as.data.frame(train_data[1:1000,])
X <- as.data.frame(train_data[1:1000,])
# X <- convert(train_data[1:1000,], to = "data.frame")
# the first column is doc_id so I remove tat
X <- as.matrix(X[,2:ncol(X)])
# X <- data.frame(y = as.character(train_Y), X)
X <- data.frame(y = as.character(train_Y[1:1000]), X)
dim(X)
library(rpart)
# Create a decision tree model
m.tree <- rpart( y ~ .,
data = X ,
minsplit = 50, #	the minimum number of observations that must exist in a node in order for a split to be attempted.
cp=.03)
print (m.tree)
dim(X)
library(rpart)
# Create a decision tree model
m.tree <- rpart( y ~ .  , ## |I am using all columns as predictors/independent variable
data = X ,
minsplit = 50, #	the minimum number of observations that must exist in a node in order for a split to be attempted.
cp=.03)
print (m.tree)
# left is yes, and right is no
plot(m.tree, uniform=TRUE)
text(m.tree, use.n=TRUE, all=TRUE, cex=.6)
X_val <-  convert(val_data, to = "data.frame")
## there is some weird name errors during transforamtion
## coerce the column names of X_val to be the same of the training data X
# table(names(X) == names(X_val))
colnames(X_val) <- c("doc_id", colnames(X)[2:ncol(X)])
pred_y_decisiontree <- predict(m.tree, X_val, type = "class")
F1(pred_y_decisiontree, val_Y)
head(pred_y_decisiontree)
library(randomForest)
m.rf = randomForest(y ~., data=X, ntree=2, nodesize = 20, proximity=F)
dim(X)
table(y)
m.rf
mean(y)
mean(X$y)
table(X$y)
str(X$y)
doc.term
library(quanteda)
library(Rtsne)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
# text to corpus
twcorpus <- corpus(reviews$text)
# the below commented lien won't work; read the help file
# doc.term <- dfm(twcorpus,  ngrams=1:3, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE,  verbose=TRUE, remove=c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"))
# transform corpus to tokens
tokens <-  tokens(twcorpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE)
tokens <- tokens_remove(tokens, c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br", " "))
tokens <- tokens_wordstem(tokens)
tokens <- tokens_ngrams(tokens, n = 1, concatenator = ".")
# then generate document-
doc.term <- dfm(tokens,  verbose=TRUE)
print (dim(doc.term))
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=500, verbose=TRUE)
print (dim(doc.term))
dim(X)
library(quanteda)
library(Rtsne)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
# text to corpus
twcorpus <- corpus(reviews$text)
# the below commented lien won't work; read the help file
# doc.term <- dfm(twcorpus,  ngrams=1:3, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE,  verbose=TRUE, remove=c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"))
# transform corpus to tokens
tokens <-  tokens(twcorpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE)
tokens <- tokens_remove(tokens, c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br", " "))
tokens <- tokens_wordstem(tokens)
tokens <- tokens_ngrams(tokens, n = 1, concatenator = ".")
# then generate document-
doc.term <- dfm(tokens,  verbose=TRUE)
print (dim(doc.term))
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=500, verbose=TRUE)
print (dim(doc.term))
X <- as.matrix(doc.term)
dim(X)
doc.term
doc.term[,2]
library(gmodels)
X_subset = t(X[1:1500, ])
pc <- fast.prcomp(X_subset, retx = TRUE, center = TRUE, scale. = FALSE, tol = NULL)
# you can take first several principal components in this way
pca_vector <- pc$x[,1:2]
pca_vector
plot(pca_vector)
plot(pca_vector)
pca_vector[pca_vector[,1]>80,]
pca_vector[pca_vector[,1]>30,]
plot(pca_vector)
tsne <- Rtsne(X_subset, dims = 2, partial_pca = T, perplexity=30, verbose=TRUE, max_iter = 50)
tsne <- Rtsne(X_subset, dims = 2, partial_pca = T, perplexity=30, verbose=TRUE, max_iter = 50)
embedding <- tsne$Y
plot(embedding)
tsne <- Rtsne(X_subset, dims = 2, partial_pca = T, perplexity=30, verbose=TRUE, max_iter = 50)
embedding <- tsne$Y
plot(embedding)
mx1 <-which.max(embedding[,1])
embedding[mx1,]
colnames(X)[mx1]
mx2 <-which.max(embedding[,2])
embedding[mx2,]
colnames(X)[mx2]
plot(embedding)
mx1 <-which.max(embedding[,1])
embedding[mx1,]
colnames(X)[mx1]
mx2 <-which.max(embedding[,2])
embedding[mx2,]
colnames(X)[mx2]
mx2 <-which.min(embedding[,2])
embedding[mx2,]
colnames(X)[mx2]
doc.term
# you can take first several principal components in this way
pca_vector <- pc$x[,1:2]
plot(pca_vector)
pca_vector[pca_vector[,1]>30,]
# you can take first several principal components in this way
pca_vector <- pc$x[,1:2]
plot(pca_vector)
pca_vector[pca_vector[,1]>30,]
doc.term
library(quanteda)
library(Rtsne)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
reviews$text
library(word2vec)
model <- word2vec(reviews[1:100,]$text, type = "cbow", dim = 15, iter = 20)
embedding <- as.matrix(model)
head(embedding)
lookslike <- predict(model, c("completely", "absolutely"), type = "nearest", top_n = 5)
lookslike
doc.term
library(quanteda)
library(Rtsne)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
library(quanteda)
library(Rtsne)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics-codes/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
# text to corpus
twcorpus <- corpus(reviews$text)
# the below commented lien won't work; read the help file
# doc.term <- dfm(twcorpus,  ngrams=1:3, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE,  verbose=TRUE, remove=c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"))
# transform corpus to tokens
tokens <-  tokens(twcorpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE)
tokens <- tokens_remove(tokens, c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br", " "))
tokens <- tokens_wordstem(tokens)
tokens <- tokens_ngrams(tokens, n = 1, concatenator = ".")
# then generate document-
doc.term <- dfm(tokens,  verbose=TRUE)
print (dim(doc.term))
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=500, verbose=TRUE)
print (dim(doc.term))
X <- as.matrix(doc.term)
library(gmodels)
X_subset = t(X[1:1500, ])
pc <- fast.prcomp(X_subset, retx = TRUE, center = TRUE, scale. = FALSE, tol = NULL)
# you can take first several principal components in this way
pca_vector <- pc$x[,1:2]
plot(pca_vector)
pca_vector[pca_vector[,1]>30,]
tsne <- Rtsne(X_subset, dims = 2, partial_pca = T, perplexity=30, verbose=TRUE, max_iter = 50)
embedding <- tsne$Y
plot(embedding)
mx1 <-which.max(embedding[,1])
embedding[mx1,]
colnames(X)[mx1]
mx2 <-which.min(embedding[,2])
embedding[mx2,]
colnames(X)[mx2]
library(quanteda)
library(Rtsne)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
library(quanteda)
library(Rtsne)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics-codes/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
library(word2vec)
set.seed(123456789)
model <- word2vec(reviews[1:100,]$text, type = "cbow", dim = 15, iter = 20)
embedding <- as.matrix(model)
# embedding <- predict(model, c("completely", "abs"), type = "embedding")
lookslike <- predict(model, c("completely", "absolutely"), type = "nearest", top_n = 5)
lookslike
