{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the previous tutorial, we have seen how you can use this package to do topic modeling.\n",
    "\n",
    "\n",
    "We are still using `gensim` package to do word embedding\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print (common_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a simpliest word embedding by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.5397789e-03 -3.2282274e-03 -1.2569737e-03  7.2841393e-04\n",
      "  1.2741253e-03  2.8842513e-03 -2.4538881e-03  2.1840320e-03\n",
      " -3.8623079e-03  3.1025445e-03 -2.3015030e-03 -2.6509964e-03\n",
      " -1.4125557e-03 -1.4433791e-03 -3.2573575e-05 -2.8911850e-03\n",
      " -1.7776939e-03  1.6191918e-03 -4.9340292e-03  2.0010355e-03\n",
      " -4.6161762e-03  4.6482263e-03  3.7053381e-03  4.1462135e-04\n",
      " -2.4262171e-03  1.6118487e-03 -1.7214738e-03  3.4235239e-03\n",
      " -1.4713259e-03 -4.6544475e-03  1.1935809e-03 -4.8896810e-03\n",
      " -4.5390655e-03  3.9357250e-03  3.1243931e-03  4.4861599e-03\n",
      "  1.4390240e-03 -4.7964332e-04 -4.1267099e-03  4.6773674e-03\n",
      " -1.7763444e-03 -3.2093960e-03 -4.4729011e-03  3.6501582e-03\n",
      " -4.9144891e-03  1.0931130e-03  1.8309371e-03  1.8265968e-03\n",
      " -9.3334389e-04 -3.8032245e-03  3.8003726e-03  3.8369901e-03\n",
      "  4.1592662e-04 -4.7283201e-03 -2.3865670e-03 -3.6414596e-03\n",
      " -6.2452385e-04 -2.5991260e-03  1.0658320e-03  1.4847112e-03\n",
      " -4.5525501e-03  3.4132805e-03  1.2193366e-03 -2.0629566e-03\n",
      "  2.6368166e-04  4.4229394e-03  2.8634754e-03  9.1193512e-04\n",
      "  3.7484986e-03  1.1347925e-03  3.9139003e-03 -8.4773032e-04\n",
      "  6.7627063e-04  4.8551750e-03  3.4956385e-03 -1.4304264e-03\n",
      "  3.3148555e-03  4.6994723e-03 -2.9563904e-03 -4.2945761e-03\n",
      "  3.7491231e-03 -3.9668288e-03  3.7116760e-03  2.1046654e-03\n",
      " -3.8434234e-03  1.1110620e-03  3.7647986e-03 -1.3135843e-03\n",
      " -3.9211521e-03  5.6734571e-04 -4.2785374e-03  4.2625589e-04\n",
      " -4.5605316e-03 -1.7758720e-03  2.4711494e-03  1.8273218e-03\n",
      "  3.0011081e-03  3.8883414e-03 -2.6083323e-03 -3.9670137e-03]\n"
     ]
    }
   ],
   "source": [
    "path = get_tmpfile(\"word2vec.model\")\n",
    "\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "vector = model.wv['computer']  # numpy vector of a word\n",
    "print (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Word2VecKeyedVectors' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9134bc340c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# new words? cannot handle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Word2VecKeyedVectors' object is not callable"
     ]
    }
   ],
   "source": [
    "# new words? cannot handle\n",
    "print (model.wv(['hi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read pre-trained models\n",
    "\n",
    "As we said, it's usually far more better to use some pre-trained embeddings instead of starting from scratches\n",
    "\n",
    "Read it more here\n",
    "\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "\n",
    "\n",
    "I will read GloVe's pre-trained vectors here. Gensim offers download of some other pre-trained vectors. See it here\n",
    "https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "\n",
    "And a more complete pre-trained vector dataset can be found here\n",
    "http://vectors.nlpl.eu/repository/\n",
    "\n",
    "You may need to manually download them to your disk and let Gensim read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 0.847267210483551), ('man', 0.832349419593811), ('mother', 0.827568769454956), ('boy', 0.7720510959625244), ('she', 0.7632068395614624), ('child', 0.7601762413978577), ('wife', 0.7505022287368774), ('her', 0.7445706129074097), ('herself', 0.7426273822784424), ('daughter', 0.7264456748962402)]\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar(positive=['woman'])\n",
    "print (result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may observe, girl and man are really different things. So we can use the king/queen and man/women analogy to find what's the similar word to \"woman\", if we hope to find pairs such as (king, queen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('man', 0.7907768487930298), ('father', 0.722454309463501), ('son', 0.7012036442756653), ('boy', 0.6988654732704163), ('another', 0.6853840351104736), ('person', 0.6754911541938782), ('who', 0.6728038191795349), ('brother', 0.6645877957344055), ('mother', 0.6613163948059082), ('one', 0.6554157733917236)]\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar(positive=[ 'king', 'woman'], negative=['queen'])\n",
    "print (result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
