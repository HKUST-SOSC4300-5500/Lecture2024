library(quanteda)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
# text to corpus
twcorpus <- corpus(reviews$text)
# the below commented lien won't work; read the help file
# doc.term <- dfm(twcorpus,  ngrams=1:3, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE,  verbose=TRUE, remove=c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"))
# transform corpus to tokens
tokens <-  tokens(twcorpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE)
tokens <- tokens_remove(tokens, c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br", " "))
tokens <- tokens_wordstem(tokens)
tokens <- tokens_ngrams(tokens, n = 1:2, concatenator = ".")
# then generate document-
doc.term <- dfm(tokens,  verbose=TRUE)
print (dim(doc.term))
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=50, verbose=TRUE)
print (dim(doc.term))
# X <- convert(doc.term, to = "data.frame")
# # the first column is doc_id so I remove tat
# X <- X[,2:ncol(X)]
# not run; too slow and memory intensive
# m.lm <- lm (reviews$label ~ as.matrix(X))
library(glmnet)
m.lasso <- glmnet(doc.term, reviews$label,
family="binomial", lambda = 0.1, alpha=1, intercept=TRUE)
m.lasso <- glmnet(doc.term, reviews$label,
family="binomial", nlambda = 20, alpha=1, intercept=TRUE)
print (m.lasso, nlambda = 20)
plot(m.lasso, xvar = "lambda", label = TRUE)
n_train <- round(nrow(doc.term) * 0.8)
train_index <- sample(1:nrow(doc.term), n_train, replace = FALSE)
val_index <- (1:nrow(doc.term))[-train_index]
train_data <- doc.term[train_index, ]
val_data <- doc.term[val_index, ] # same as df[-train, ]
train_Y <- reviews[train_index]$label
val_Y <- reviews[val_index]$label
## function to compute accuracy
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
F1 <- function(ypred, y){
prec <- precision (ypred, y)
rec <- recall (ypred, y)
return ( ( 2 * prec * rec) / (prec + rec))
}
lambda_list <- seq(0,0.01,0.002)
F1.list <- c()
## notice that I have used the newer training data for fitting this model
for (each_lambda in lambda_list){
m.lasso <- glmnet(train_data, train_Y,
family="binomial", lambda = each_lambda, alpha=1)
# predict and evaluate on the validation data
pred_y <- predict(m.lasso, newx = val_data, type="class")
head(pred_y)
F1.list <- c(F1.list, F1(pred_y, val_Y))
}
plot(lambda_list, F1.list)
# use the original training data (without splitting to new train/validation)
# the package will do that for you.
cv.glmnet(doc.term, reviews$label, family="binomial", type.measure = "auc", alpha = 1, nfolds = 5)
m.lasso <- glmnet(train_data, train_Y,
family="binomial", lambda = 0.001854, alpha=1)
m.lasso$beta
pos.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/positive-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
neg.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/negative-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
mydict <- dictionary(list(positive = pos.words,
negative = neg.words))
sent <- dfm(doc.term, dictionary = mydict)
sent_val <- sent[val_index, ]
pred_y_dict <- ifelse(as.numeric(sent_val[,1]) > as.numeric(sent_val[,2]), 1, 0)
F1(pred_y_dict, val_Y)
X <- as.data.frame(train_data[1:1000,])
# X <- convert(train_data[1:1000,], to = "data.frame")
# the first column is doc_id so I remove tat
X <- as.matrix(X[,2:ncol(X)])
# X <- data.frame(y = as.character(train_Y), X)
X <- data.frame(y = as.character(train_Y[1:1000]), X)
library(rpart)
# Create a decision tree model
m.tree <- rpart( y ~ .,
data = X ,
cp=.03)
print (m.tree)
# left is yes, and right is no
plot(m.tree, uniform=TRUE)
text(m.tree, use.n=TRUE, all=TRUE, cex=.6)
# library(rattle)
# fancyRpartPlot(m.tree, caption = NULL)
X_val <-  as.data.frame(val_data)
## tehre is some weird name errors during transforamtion
## coerce the column names of X_val to be the same of the training data X
# table(names(X) == names(X_val))
colnames(X_val) <- c("doc_id", colnames(X)[2:ncol(X)])
pred_y_decisiontree <- predict(m.tree, X_val, type = "class")
F1(pred_y_decisiontree, val_Y)
library(randomForest)
m.rf = randomForest(y ~., data=X, ntree=2, nodesize = 20, proximity=F)
library(doParallel)
library("foreach")
cl <- makeCluster(detectCores())
registerDoParallel(cl)
m.rf.parallel <- foreach(ntree = rep(5, detectCores()), .combine = combine, .packages = "randomForest") %dopar% {
randomForest(x = X[,2:ncol(X)], y = X$y, ntree = ntree, nodesize = 20, proximity=F)
}
library(e1071)
# use only 1000 training data
m.svm <- svm(y~., X,  kernel =
"linear")
str(X)
library(e1071)
# use only 1000 training data
m.svm <- svm(as.numeric(y) ~., X,  kernel =
"linear")
# use full data if you have more resources.
# m.svm <- svm(x = doc.term, y = reviews$label, kernel ="linear")
summary(m.svm)
# test with train data
pred_y_svm <- predict(m.svm, X_val)
F1(pred_y_svm, val_Y)
table(val_Y)
table(pred_y_svm)
# use only 1000 training data
m.svm <- svm(y ~., X,  kernel =
"linear")
# use only 1000 training data
m.svm <- svm(as.factor(y) ~., X,  kernel =
"linear")
# test with train data
pred_y_svm <- predict(m.svm, X_val)
F1(pred_y_svm, val_Y)
head(reviews)
print (dim(doc.term))
doc.term
type(doc.term)
class(doc.term)
lm (reviews$label ~ doc.term)
help(lm)
# X <- convert(doc.term, to = "data.frame")
X <- as.data.frame(doc.terms)
# X <- convert(doc.term, to = "data.frame")
X <- as.data.frame(doc.term)
rm(X)
help("glmnet")
library(glmnet)
m.lasso <- glmnet(doc.term, reviews$label,
family="binomial", lambda = 0.1, alpha=1, intercept=TRUE)
m.lasso$beta
print (m.lasso, nlambda = 20)
m.lasso <- glmnet(doc.term, reviews$label,
family="binomial", nlambda = 20, alpha=1, intercept=TRUE)
print (m.lasso, nlambda = 20)
plot(m.lasso, xvar = "lambda", label = TRUE)
n_train <- round(nrow(doc.term) * 0.8)
n_train
head(train_index )
head(val_index)
lambda_list <- seq(0,0.01,0.002)
lambda_list
m.lasso <- glmnet(train_data, train_Y,
family="binomial", lambda = each_lambda, alpha=1)
# predict and evaluate on the validation data
pred_y <- predict(m.lasso, newx = val_data, type="class")
head(pred_y)
each_lambda
dim(pred_y)
F1(pred_y, val_Y)
plot(lambda_list, F1.list)
lambda_list <- seq(0,0.01,0.002)
F1.list <- c()
## notice that I have used the newer training data for fitting this model
for (each_lambda in lambda_list){
m.lasso <- glmnet(train_data, train_Y,
family="binomial", lambda = each_lambda, alpha=1)
# predict and evaluate on the validation data
pred_y <- predict(m.lasso, newx = val_data, type="class")
head(pred_y)
F1.list <- c(F1.list, F1(pred_y, val_Y))
}
plot(lambda_list, F1.list)
help("cv.glmnet")
# use the original training data (without splitting to new train/validation)
# the package will do that for you.
cv.glmnet(doc.term, reviews$label, family="binomial", type.measure = "auc", alpha = 1, nfolds = 5)
m.lasso <- glmnet(train_data, train_Y,
family="binomial", lambda = 0.001854, alpha=1)
m.lasso$beta
pos.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/positive-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
sent <- dfm(doc.term, dictionary = mydict)
sent_val <- sent[val_index, ]
pred_y_dict <- ifelse(as.numeric(sent_val[,1]) > as.numeric(sent_val[,2]), 1, 0)
F1(pred_y_dict, val_Y)
dim(train_data)
X <- as.data.frame(train_data[1:1000,])
# X <- convert(train_data[1:1000,], to = "data.frame")
# the first column is doc_id so I remove tat
X <- as.matrix(X[,2:ncol(X)])
# X <- data.frame(y = as.character(train_Y), X)
X <- data.frame(y = as.character(train_Y[1:1000]), X)
View(X)
library(rpart)
# Create a decision tree model
m.tree <- rpart( y ~ .,
data = X ,
cp=.03)
print (m.tree)
# left is yes, and right is no
plot(m.tree, uniform=TRUE)
text(m.tree, use.n=TRUE, all=TRUE, cex=.6)
plot(m.tree, uniform=TRUE)
text(m.tree, use.n=TRUE, all=TRUE, cex=.6)
help(m.tree)
help(rpart)
X_val <-  as.data.frame(val_data)
## tehre is some weird name errors during transforamtion
## coerce the column names of X_val to be the same of the training data X
# table(names(X) == names(X_val))
colnames(X_val) <- c("doc_id", colnames(X)[2:ncol(X)])
pred_y_decisiontree <- predict(m.tree, X_val, type = "class")
F1(pred_y_decisiontree, val_Y)
help("randomForest")
detectCores()
rep(5, detectCores())
help("svm")
library(e1071)
# use only 1000 training data
m.svm <- svm(as.factor(y) ~., X,  kernel =
"linear")
# use full data if you have more resources.
# m.svm <- svm(x = doc.term, y = reviews$label, kernel ="linear")
summary(m.svm)
# test with train data
pred_y_svm <- predict(m.svm, X_val)
F1(pred_y_svm, val_Y)
library(quanteda)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
View(reviewes)
View(reviews)
# text to corpus
twcorpus <- corpus(reviews$text)
# transform corpus to tokens
tokens <-  tokens(twcorpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE)
stopwords("english")
tokens <- tokens_remove(tokens, c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br", " "))
tokens <- tokens_wordstem(tokens)
tokens <- tokens_wordstem(tokens)
tokens <- tokens_ngrams(tokens, n = 1:2, concatenator = ".")
tokens
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=50, verbose=TRUE)
# then generate document-
doc.term <- dfm(tokens,  verbose=TRUE)
# and discard too infrequent words;
doc.term <- dfm_trim(doc.term, min_docfreq=50, verbose=TRUE)
print (dim(doc.term))
nrow(doc.term)
n_train <- round(nrow(doc.term) * 0.7)
n_train
# n_train is the number of training data
n_test <- round (nrow(doc.term) * 0.2)
n_test
sample(1:10, 5, replace = True)
sample(1:10, 5, replace = T)
sample(1:10, 5, replace = FALSE)
# first set up test data
test_index <- sample(1:nrow(doc.term), n_test, replace = FALSE)
head(test_index)
1:10[-2]
(1:10)[-c(1,2)]
# what is left is the training and validation data
train_and_val_index <- (1:nrow(doc.term))[-test_index]
length(train_and_val_index)
n_train
# further sample training data within (train and validation data)
train_index <- sample(train_and_val_index, n_train, replace = FALSE)
head(train_index)
# the rest is test data
val_index <- setdiff(train_and_val_index, train_index)
## get train, val, and test data
test_data <- doc.term[test_index, ]
head(test_index)
## get train, val, and test data
test_data <- doc.term[test_index, ]
train_data <- doc.term[train_index, ]
val_data <- doc.term[val_index, ]
train_and_val_data <- doc.term[train_and_val_index, ]
test_Y <- reviews[test_index]$label
train_Y <- reviews[train_index]$label
val_Y <- reviews[val_index]$label
train_and_val_Y <- reviews[train_and_val_index]$label
head(train_Y)
head(train_data)
# X <- convert(doc.term, to = "data.frame")
# X <- as.data.frame(doc.term)
# # the first column is doc_id so I remove tat
# X <- X[,2:ncol(X)]
## the first column is doc_id so I remove tat
X <- X[,2:ncol(X)]
X <- convert(doc.term, to = "data.frame")
X <- as.data.frame(doc.term)
## the first column is doc_id so I remove tat
X <- X[,2:ncol(X)]
head(X)
dim(X)
View(X)
library(glmnet)
m.lasso <- glmnet(train_and_val_data, train_and_val_Y,
family="binomial", lambda = 0.1, alpha=1, intercept=TRUE)
help("glmnet")
m.lasso <- glmnet(train_and_val_data, train_and_val_Y,
family="binomial", nlambda = 20, alpha=1, intercept=TRUE)
print (m.lasso, nlambda = 20)
dim(train_and_val_data)
11684 - 8150
plot(m.lasso, xvar = "lambda", label = TRUE)
train_and_val_data[,360]
summary(m.lasso)
m.lasso$beta
m.lasso$beta
m.lasso <- glmnet(train_and_val_data, train_and_val_Y,
family="binomial", lambda = 0.1, alpha=1, intercept=TRUE)
m.lasso$beta
# print all regression coefficients here
m.lasso$beta
library(glmnet)
m.lasso <- glmnet(train_and_val_data, train_and_val_Y,
family="binomial", lambda = 0.15, alpha=1, intercept=TRUE)
# print all regression coefficients here
m.lasso$beta
library(glmnet)
m.lasso <- glmnet(train_and_val_data, train_and_val_Y,
family="binomial", lambda = 0.01, alpha=1, intercept=TRUE)
# print all regression coefficients here
m.lasso$beta
library(glmnet)
m.lasso <- glmnet(train_and_val_data, train_and_val_Y,
family="binomial", lambda = 0.01, alpha=1, intercept=TRUE)
# print all regression coefficients here
# m.lasso$beta
m.lasso <- glmnet(train_and_val_data, train_and_val_Y,
family="binomial", nlambda = 20, alpha=1, intercept=TRUE)
print (m.lasso, nlambda = 20)
m.lasso$beta
m.lasso$lambda
#m.lasso$beta
plot(m.lasso, xvar = "lambda", label = TRUE)
m.lasso$beta
lambda_list <- seq(0,0.01,0.002)
lambda_list
each_lambda = 0.002
m.lasso <- glmnet(train_data, train_Y,
family="binomial", lambda = each_lambda, alpha=1)
# predict and evaluate on the validation data
pred_y <- predict(m.lasso, newx = val_data, type="class")
head(pred_y)
F1(pred_y, val_Y)
## function to compute accuracy
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
F1 <- function(ypred, y){
prec <- precision (ypred, y)
rec <- recall (ypred, y)
return ( ( 2 * prec * rec) / (prec + rec))
}
F1.list <- c(F1.list, F1(pred_y, val_Y))
F1.list <- c()
F1.list <- c(F1.list, F1(pred_y, val_Y))
F1.list
lambda_list <- seq(0,0.01,0.002)
F1.list <- c()
## notice that I have used the newer training data for fitting this model
for (each_lambda in lambda_list){
m.lasso <- glmnet(train_data, train_Y,
family="binomial", lambda = each_lambda, alpha=1)
# predict and evaluate on the validation data
pred_y <- predict(m.lasso, newx = val_data, type="class")
head(pred_y)
F1.list <- c(F1.list, F1(pred_y, val_Y))
}
plot(lambda_list, F1.list)
# use the original training data (without splitting to new train/validation)
# the package will do that for you.
cv.glmnet(doc.term, reviews$label, family="binomial", type.measure = "auc", alpha = 1, nfolds = 5)
# use the original training data (without splitting to new train/validation)
# the package will do that for you.
cv.glmnet(train_val_data, train_val_Y, family="binomial", type.measure = "auc", alpha = 1, nfolds = 5)
train_and_val_data
# use the original training data (without splitting to new train/validation)
# the package will do that for you.
cv.glmnet(train_and_val_data, train_and_val_Y, family="binomial", type.measure = "auc", alpha = 1, nfolds = 5)
m.lasso$beta
m.lasso <- glmnet(test_data, test_Y,
family="binomial", lambda = 0.002217, alpha=1)
m.lasso$beta
m.lasso <- glmnet(test_data, test_Y,
family="binomial", lambda = 0.002217, alpha=1)
# m.lasso$beta
pos.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/positive-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
neg.words <- read.csv("../lec4-text-basics/opinion-lexicon-English/negative-words.txt",
stringsAsFactors = FALSE, header = FALSE)$V1
mydict <- dictionary(list(positive = pos.words,
negative = neg.words))
sent <- dfm(doc.term, dictionary = mydict)
sent_val <- sent[val_index, ]
pred_y_dict <- ifelse(as.numeric(sent_val[,1]) > as.numeric(sent_val[,2]), 1, 0)
F1(pred_y_dict, val_Y)
X <- as.data.frame(train_data[1:1000,])
X <- as.data.frame(train_data[1:1000,])
# X <- convert(train_data[1:1000,], to = "data.frame")
# the first column is doc_id so I remove tat
X <- as.matrix(X[,2:ncol(X)])
# X <- data.frame(y = as.character(train_Y), X)
X <- data.frame(y = as.character(train_Y[1:1000]), X)
head(X)
View(X)
help(rpart)
library(rpart)
help(rpart)
library(rpart)
# Create a decision tree model
m.tree <- rpart( y ~ .,
data = X ,
minsplit = 50, #	the minimum number of observations that must exist in a node in order for a split to be attempted.
cp=.03)
print (m.tree)
# left is yes, and right is no
plot(m.tree, uniform=TRUE)
text(m.tree, use.n=TRUE, all=TRUE, cex=.6)
library(rattle)
library(rattle)
install.packages("rattle")
X_val <-  convert(val_data, to = "data.frame")
## there is some weird name errors during transforamtion
## coerce the column names of X_val to be the same of the training data X
# table(names(X) == names(X_val))
colnames(X_val) <- c("doc_id", colnames(X)[2:ncol(X)])
pred_y_decisiontree <- predict(m.tree, X_val, type = "class")
F1(pred_y_decisiontree, val_Y)
library(randomForest)
library(randomForest)
detectCores()
library(doParallel)
detectCores()
library(quanteda)
library(data.table) # this package help you to read large files quicker
reviews <- fread("../lec4-text-basics/train.csv") # fread() is a function in data.table
reviews$label <- reviews$label # coerce 0/1 to category
# text to corpus
twcorpus <- corpus(reviews$text)
# the below commented lien won't work; read the help file
# doc.term <- dfm(twcorpus,  ngrams=1:3, tolower=TRUE, stem=TRUE, remove_punct = TRUE, remove_url=TRUE,  verbose=TRUE, remove=c(stopwords("english"), "the", "t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "<", ">", "br"))
# transform corpus to tokens
tokens <-  tokens(twcorpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE)
